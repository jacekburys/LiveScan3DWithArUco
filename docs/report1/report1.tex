\documentclass[a4paper,12pt]{article}
\usepackage [utf8]{inputenc}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.12,0.37,0.12}
\definecolor{darkblue}{rgb}{0,0,0.5}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{black},
  keywordstyle=\color{darkblue},
  commentstyle=\color{gray},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand*\justify{%
  \fontdimen2\font=0.4em% interword space
  \fontdimen3\font=0.2em% interword stretch
  \fontdimen4\font=0.1em% interword shrink
  \fontdimen7\font=0.1em% extra space
  \hyphenchar\font=`\-% allowing hyphenation
}

\addtolength{\hoffset}{-1,5cm}
\addtolength{\textwidth}{3cm}

\title{LiveScan 3D With ArUco Markers}
\author{Rui Liu (rl2414), Adam Hosier (ah3114), Jacek Burys (jsb314)\\Ayman Moussa (am5514), Kabeer Vohra (kv113)}
\date{\today}

\begin{document}

\maketitle

\iffalse
The intent of this project is to introduce an improvement on an open source real time 3D reconstruction library, LiveScan3D (https://github.com/MarekKowalski/LiveScan3D).
LiveScan3D is a system designed for 3D reconstruction using multiple Kinect v2 depth sensors simultaneously in real time.
This library contains code for calibration of the relative postitions of different sensors. However, this part is a bit cumbersome.
The goal is to use another library, ArUco (http://www.uco.es/investiga/grupos/ava/node/26) to simplify, and potentually improve the accuracy of the calibration process. 
\fi

\section*{Introduction}

The goal of this project is to improve an existing open source project,
LiveScan3D, which is used to reconstruct 3D objects based on input from 
multiple Kinect v2 depth sensors. The reconstruction happens in real-time 
and the result is a point cloud. 
We are going to improve the calibration of LiveScan3D. Currently this step is
done by using a piece of paper with a pattern that the software can
recognize and determine its position. This step can be a bit cumbersome,
especially in unusual locations. 
We believe that we could use another open source project to solve this problem.
ArUco would give us the ability to recognise multiple markers, and therefore
could be used for more accurate calibration of LiveScan3D. We also have had
initial ideas about placing the markers on a 3D object, for example a cube,
and use it for calibration. \\

In conclusion, we believe this project can contribute to LiveScan3D in terms of
it's modularity and potentially it's accuracy.

\section*{Methods}
For this project we have decided to follow the Extreme Programming (XP) method of agile development. The reasons for this are:

\begin{itemize}
\item Extreme programming is split up into iterations which works well with the checkpoints we need to deliver throughout the project. We will schedule smaller iterations in between the project checkpoint cycles to ensure we are on track to meet the next larger checkpoint.
\item Pair programming will also be useful because we need to be working with the hardware which will be in labs so we can maximise the time we have with the hardware and work together rather than remotely.
\item Creating an open work space for extreme programming will be easier to achieve while doing pair programming as we will need to work with the Kinect camera hardware so we will likely be meeting to work together in labs for large portions of the project.
\item Ensuring the customer is always available is also convenient for this project as our customer is our supervisor. We will do requirement capture via user stories from our supervisor in order to help guide the development and ensure all conditions are met.
\item Daily stand-up meetings will be possible while we are in university in order to keep track of progress within the project and ensure we are on track for delivering the requirements.
\end{itemize}

\section*{Planning}
We plan to divide the work into parts that need to be completed with the hardware and parts that can be completed without. Any work that requires the Kinect sensors will need to be carried out on the appropriate lab machines and we plan to gather as a group to discuss and delegate this work in frequent stand-up meetings. Some work will be able to be done without the sensors, thus giving us more flexibility when it comes to organising it, as group members can complete the work in their own time. \\

TODO: talk about requirements and how we will capture those. \\
\\
Speaking to our supervisor gave us some insight into the scope of the project, the programming environments needed to carry out the work, as well as suggestions that helped us put together the following iteration plan:

\subsection*{Iteration 1}
For the first iteration we plan to familiarise ourselves with the hardware obtained from CSG; two Kinect v2 sensors. We will also read through the code and understand the relevant sections of the LiveScan3D and ArUco repositories on GitHub. Both projects have demos that we will have to run to better understand how they work. We will then start experimenting with the code and figuring out where we could put our changes.
There are some constraints with LiveScan3D as it only works on USB 3.0 and is currently restricted to Windows. To overcome these problems we will identify and build environments that can suitibly run the LiveScan3D software with the Kinect sensors, and work on these environments when needed. Not all work will require the hardware, and this work can be carried out on many more machines.
\\

\subsection*{Iteration 2}
\iffalse
The calibration process in LiveScan3D is currently a bit cumbersome. The idea is to use ArUco to simplify this, and also possibly make it more accurate. Yes, you can use multiple markers, maybe even arranged on 3D objects like cubes. We can discuss details on Wednesday. A simplified calibration process would be important in the context of using something like LiveScan3D as a portable 3D recording tool.
\fi
At this point we expect we will have a good understanding of both LiveScan3D and ArUco having read and understood the code. We will start implementing our changes by getting the calibration process to work with both sensors using one ArUco marker. A simplified calibration process is important in the context of using LiveScan3D as a portable 3D recording tool. If we have time at the end, we may try to make the calibration more accurate.  

\subsection*{Iteration 3}
When our calibration process works with one ArUco marker, we plan to try to experiment with the idea of placing multiple markers on a cube. \\
// TODO : more text

\subsection*{Iteration 4}
In the final iteration we would like to complete an extension suggested by our supervisor. LiveScan3D can already record the objects and store them as a list of vertices in a PLY file. We would like to take this further and and an option in the application to extract meshes from the point cloud. This could give us 3D objects of better visual quality that can be opened and edited in third party applications. 
\end{document}
