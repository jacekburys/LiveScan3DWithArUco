\documentclass[a4paper,12pt]{article}
\usepackage [utf8]{inputenc}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.12,0.37,0.12}
\definecolor{darkblue}{rgb}{0,0,0.5}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{black},
  keywordstyle=\color{darkblue},
  commentstyle=\color{gray},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand*\justify{%
  \fontdimen2\font=0.4em% interword space
  \fontdimen3\font=0.2em% interword stretch
  \fontdimen4\font=0.1em% interword shrink
  \fontdimen7\font=0.1em% extra space
  \hyphenchar\font=`\-% allowing hyphenation
}

\addtolength{\hoffset}{-1,5cm}
\addtolength{\textwidth}{3cm}

\title{LiveScan 3D With ArUco Markers}
\author{Rui Liu (rl2414)\\Adam Hosier (ah3114)\\Jacek Burys (jsb314)\\Ayman Moussa (am5514)\\Kabeer Vohra (kv113)}
\date{\today}

\begin{document}

\maketitle
Introduction

The intent of this project is to introduce an improvement on an open source real time 3D reconstruction library - LiveScan3D (https://github.com/MarekKowalski/LiveScan3D).
LiveScan3D is a system designed for 3D reconstruction using multiple Kinect v2 depth sensors simultaneously at real time speed.
This library contains code for calibration on the relative postitions of different sensors. However, this part is a bit cumbersome.
The goal is to use another library ArUco (http://www.uco.es/investiga/grupos/ava/node/26) to simplify some of the calibration process. It's also possible to get it more accurate potentially.

\section*{Methods}
For this project we have decided to follow the Extreme Programming (XP) agile method of development. The reasons for this are:

\begin{itemize}
\item Extreme programming is split up into iterations which works well with the checkpoints we need to deliver throughout the project. We will do smaller iterations in between the project checkpoint cycles to ensure we are on track to meet the next larger checkpoint.
\item Pair programming will be useful as well because we need to be working with the hardware which will be in labs so we can maximise the time we have with the hardware and work together rather than remotely.
\item Creating an open work space for extreme programming will be easier to achieve while doing pair programming as we will need to work with the Kinect camera hardware so we will likely be meeting to work together in labs for large portions of the project.
\item Ensuring the customer is always available is also convenient for this project as our customer is our supervisor. We will do requirement capture via user stories from our supervisor in order to help guide the development and ensure all conditions are met.
\item Daily stand-up meetings will be possible in since we are in university in order to keep track of progress within the project and ensure we are on track for delivering the requirements.
\end{itemize}

\section*{Planning}
We plan to divide the work into parts that need to be completed with the hardware, and parts that can be completed without. Any work that requires the Kinect sensors will need to be carried out on the appropriate lab machines, and we plan to gather as a group to discuss and delegate this work. Some work will be able to be done without the sensors, thus giving us more flexibility when it comes to organising it, as group members can complete the work in their own time. \\
TODO: talk about requirements and how we will capture those. \\
Speaking to our supervisor gave us some insight into the scope of the project, the programming environments needed to carry out the work, as well as suggestions helping us to put together the iteration following plan:
\subsection*{TODO: Copied the iteration plan from email, this needs to be put into our own words}
\subsection*{Iteration 1}
Familiarize yourself with the hardware. You will be given two Kinect v2 sensors. They only work on USB 3.0, so you might need to check that you have 'reserved' the right lab machines (or your own laptops). Also, LiveScan3D is currently Windows only, I think.

\subsection*{Iteration 2/3}
My understanding is that the calibration process in LiveScan3D is a bit cumbersome. The idea is to use ArUco to simplify this, and also maybe even make it more accurate. Yes, you can use multiple markers, maybe even arranged on 3D objects like cubes. We can discuss details on Wednesday. A simplified calibration process would be important in the context of using something like LiveScan3D as a portable 3D recording tool.

\subsection*{Iteration 4}
You don't necessarily need to generate meshes from the recordings. You could keep it as a set of 3D points (probably stored in some efficient data structure). When doing playback, you simply render the points but should allow the virtual camera to change.
\end{document}
